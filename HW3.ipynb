{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Younes Shafiee 40108194\n",
    "## HW3 Monte Carlo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "Iterations= 1000\n",
    "gamma=1\n",
    "epsilon=0.1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Environment(State, Action, Goal):\n",
    "    if (State==0 and Action==-1):\n",
    "        temp=[0,0,1,1,1,1,1,1,1,1]\n",
    "        tempValue=temp[np.random.randint(10)]\n",
    "        if tempValue==0:\n",
    "            newState=6\n",
    "            Reward=10\n",
    "        else:\n",
    "            newState=5\n",
    "            Reward=-1\n",
    "    else:\n",
    "        newState=State+Action\n",
    "        if newState== Goal:\n",
    "            Reward=10\n",
    "        else:\n",
    "            Reward=-1    \n",
    "    \n",
    "    \n",
    "    return newState, Reward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllStates=np.array([0,1,2,3,4,5,6])\n",
    "Start_state= AllStates[1]\n",
    "Goal=6\n",
    "\n",
    "AllActions=np.array([-1,1])\n",
    "# index of 0 means Left side, index of 1 means Right side."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episode Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Episode_Generator( AllActions, Policy, Goal, epsilon,intitalState):\n",
    "\n",
    "    Epsilon=epsilon*100\n",
    "    # isTerminal= False\n",
    "    timeStep=0\n",
    "    slice_of_SR=np.empty((0,2))\n",
    "    States=np.array([intitalState])\n",
    "    action_array=np.array([])\n",
    "    reward_array=np.array([])\n",
    "\n",
    "\n",
    "    while True:\n",
    "\n",
    "        if States[timeStep]==Goal:\n",
    "            # isTerminal= True\n",
    "            break\n",
    "\n",
    "            \n",
    "        \n",
    "        if (Epsilon>np.random.randint(101)):\n",
    "            action= AllActions[np.random.randint(2)]\n",
    "        else:\n",
    "            action= Policy[int(States[timeStep])]\n",
    "\n",
    "        next_state, Reward=Environment(States[timeStep], action, Goal)\n",
    "        States=np.append(States ,next_state)\n",
    "        reward_array=np.append(reward_array, Reward)\n",
    "        action_array=np.append(action_array,action)\n",
    "        slice_of_SR=np.vstack((slice_of_SR, [States[timeStep], Reward]))\n",
    "\n",
    "\n",
    "        timeStep=timeStep+1\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    return States, reward_array, action_array, slice_of_SR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy improvement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolicyImprovement(V,AllStates, AllActions, Goal, gamma):\n",
    "\n",
    "    Policy=np.zeros((1,7))[0]\n",
    "\n",
    "    for state_idx, state in enumerate(AllStates):\n",
    "        Arbiter_vector=np.array([0,0])\n",
    "\n",
    "        if (state ==Goal):\n",
    "            continue\n",
    "\n",
    "        for action_idx , action in enumerate(AllActions):\n",
    "            NewState, Reward = Environment(state, action, Goal)\n",
    "            newState_idx= np.where(AllStates==NewState)[0][0]\n",
    "            Arbiter_vector[action_idx]=Reward+gamma*V[newState_idx]\n",
    "\n",
    "        temp= np.where(Arbiter_vector==np.amax(Arbiter_vector))[0]\n",
    "        index_of_max_action= temp[np.random.randint(len(temp))]\n",
    "        Policy[state_idx]= int(index_of_max_action)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return Policy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft-Policy Monte Carlo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  5.15441176 -33.40840841 -34.2513089  -33.67898833 -32.85884692\n",
      "   8.09451575   0.        ]\n"
     ]
    }
   ],
   "source": [
    "Policy=np.array([])\n",
    "for indexState, state in enumerate(AllStates):\n",
    "    Policy=np.append(Policy, AllActions[np.random.randint(2)])\n",
    "\n",
    "V=np.zeros((1,7))[0]\n",
    "CounterS=np.zeros((1,7))[0]\n",
    "\n",
    "# print(Policy[0])\n",
    "\n",
    "\n",
    "for nEpisode in range(1, Iterations):\n",
    "    intitalState= 1\n",
    "    Visited_States, visited_Reward, _ , slice_of_SR= Episode_Generator(AllActions,Policy,6,epsilon,intitalState)\n",
    "\n",
    "    for indexState, state in enumerate(AllStates):\n",
    "        if state == Goal:\n",
    "            continue\n",
    "        \n",
    "        indexOfStates=np.where(Visited_States==state)[0]\n",
    "        if (np.array_equal(indexOfStates, [])):\n",
    "            continue\n",
    "        \n",
    "\n",
    "        first_visit_Of_state= indexOfStates[0]\n",
    "\n",
    "\n",
    "        CounterS[indexState]=CounterS[indexState]+1\n",
    "\n",
    "        \n",
    "        power_variable=0\n",
    "        ReturnG=0\n",
    "        for _ in range(first_visit_Of_state, len(Visited_States)-1):\n",
    "\n",
    "            ReturnG=ReturnG+visited_Reward[first_visit_Of_state]*pow(gamma, power_variable)\n",
    "            power_variable +=1\n",
    "            first_visit_Of_state +=1\n",
    "\n",
    "        V[indexState]=V[indexState]+ (1/CounterS[indexState])*(ReturnG-V[indexState])\n",
    "\n",
    "print(V)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 1. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "Policy = PolicyImprovement(V,AllStates, AllActions, Goal, gamma)\n",
    "print(Policy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
